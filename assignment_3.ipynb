{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97b470e14726ef3628924dde59f4647a",
     "grade": false,
     "grade_id": "cell-1a02bff32a097b76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 3\n",
    "\n",
    "In this assignment we will be build a multi layer neural network and train it to classify hand-written digits into 10 classes (digits 0-9). Assignment 3 will build upon the learning from Assignment 2. We will extend Assignment 2 by introducing optmization techniques like dropout, momentum and learning_rate scheduling and use of minibatch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd5ac0db59406e661d8ef88be3baa17a",
     "grade": false,
     "grade_id": "cell-d9bd60ff8a7a5aba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#import libraries and functions to load the data\n",
    "# from digits import get_mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "import sys\n",
    "import numpy.testing as npt\n",
    "import pytest\n",
    "import random\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d8b8402ab293c269aef902b1afe21ca",
     "grade": false,
     "grade_id": "cell-7e789c2d07d0df38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Load and Visualize Data\n",
    "\n",
    "MNIST dataset contains grayscale samples of handwritten digits of size 28 $\\times$ 28. It is split into training set of 60,000 examples, and a test set of 10,000 examples. We will use the entire dataset for training. Since we plan to use minibatch gradient descent, we can work with a larger dataset and not worry if it will fit into memory. You will also see the improved speed of minibatch gradient descent compared to Assignment 2, where we used batch gradeint descent (using the entire training data as a batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ca9098028d932a01a5cc12ac78b384e",
     "grade": false,
     "grade_id": "cell-153e3e96f279c5f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'get_mnist' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b4875d215ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# We need to reshape the data everytime to match the format (d,m), where d is dimensions (784) and m is number of samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_mnist' is not defined"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "trX, trY, tsX, tsY = get_mnist()\n",
    "# We need to reshape the data everytime to match the format (d,m), where d is dimensions (784) and m is number of samples\n",
    "trX = trX.reshape(-1, 28*28).T\n",
    "trY = trY.reshape(1, -1)\n",
    "tsX = tsX.reshape(-1, 28*28).T\n",
    "tsY = tsY.reshape(1, -1)\n",
    "    \n",
    "# Lets examine the data and see if it is normalized\n",
    "print('trX.shape: ', trX.shape)\n",
    "print('trY.shape: ', trY.shape)\n",
    "print('tsX.shape: ', tsX.shape)\n",
    "print('tsY.shape: ', tsY.shape)\n",
    "print('Train max: value = {}, Train min: value = {}'.format(np.max(trX), np.min(trX)))\n",
    "print('Test max: value = {}, Test min: value = {}'.format(np.max(tsX), np.min(tsX)))\n",
    "print('Unique labels in train: ', np.unique(trY))\n",
    "print('Unique labels in test: ', np.unique(tsY))\n",
    "\n",
    "# Let's visualize a few samples and their labels from the train and test datasets.\n",
    "print('\\nDisplaying a few samples')\n",
    "visx = np.concatenate((trX[:,:50],tsX[:,:50]), axis=1).reshape(28,28,10,10).transpose(2,0,3,1).reshape(28*10,-1)\n",
    "visy = np.concatenate((trY[:,:50],tsY[:,:50]), axis=1).reshape(10,-1)\n",
    "    \n",
    "print('labels')\n",
    "print(visy)\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.axis('off')\n",
    "plt.imshow(visx, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37df5ec899c09d162a1714cd03e601a2",
     "grade": false,
     "grade_id": "cell-e77c10bfbde99f2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Reuse Code from Assignment 2\n",
    "\n",
    "There are some sections in this assignment which require you to use the code you implemented for Assignment 2. These sections need to work correctly for Assignment 3 to be successful. However, these sections will not have any points assigned to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7221fc5c8438626f55fed907bdbdeb70",
     "grade": false,
     "grade_id": "cell-36f264bd171e66a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Rectified Linear Unit-ReLU (repeated from Assignment 2:   0 points)\n",
    "\n",
    "ReLU (Rectified Linear Unit) is a piecewise linear function defined as\n",
    "\\begin{equation*}\n",
    "ReLU(Z) = \\text{max}(0,Z)\n",
    "\\end{equation*}\n",
    "\n",
    "Hint: use [numpy.maximum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "305c954ece9e4a95d802995ee253af50",
     "grade": false,
     "grade_id": "test_case1_relu_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    '''\n",
    "    Computes relu activation of input Z\n",
    "    \n",
    "    Inputs: \n",
    "        Z: numpy.ndarray (n, m) which represent 'm' samples each of 'n' dimension\n",
    "        \n",
    "    Outputs: \n",
    "        A: where A = ReLU(Z) is a numpy.ndarray (n, m) representing 'm' samples each of 'n' dimension\n",
    "        cache: a dictionary with {\"Z\", Z}\n",
    "        \n",
    "    '''\n",
    "    cache = {}\n",
    "    # your code here\n",
    "    if type(Z)==list:\n",
    "        Z = np.array(Z)\n",
    "        \n",
    "    A =np.maximum(0,Z)\n",
    "    cache = {\"Z\": Z}\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d68ac986a1737d197092d30d8807deb6",
     "grade": true,
     "grade_id": "test_case1_relu",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "z_tst = [-2,-1,0,1,2]\n",
    "a_tst, c_tst = relu(z_tst)\n",
    "npt.assert_array_equal(a_tst,[0,0,0,1,2])\n",
    "npt.assert_array_equal(c_tst[\"Z\"], [-2,-1,0,1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f49ae7f620c77d0e403a032c633e256",
     "grade": false,
     "grade_id": "cell-9e69ac398fc920e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### ReLU - Gradient (repeated from Assignment 2:   0 points)\n",
    "\n",
    "The gradient of ReLu($Z$) is 1 if $Z>0$ else it is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc140b793336a33c53c1fe2d27397f57",
     "grade": false,
     "grade_id": "test_case2_relu_der_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu_der(dA, cache):\n",
    "    '''\n",
    "    Computes derivative of relu activation\n",
    "    \n",
    "    Inputs: \n",
    "        dA: derivative from the subsequent layer of dimension (n, m). \n",
    "            dA is multiplied elementwise with the gradient of ReLU\n",
    "        cache: dictionary with {\"Z\", Z}, where Z was the input \n",
    "            to the activation layer during forward propagation\n",
    "        \n",
    "    Outputs: \n",
    "        dZ: the derivative of dimension (n,m). It is the elementwise \n",
    "            product of the derivative of ReLU and dA\n",
    "        \n",
    "    '''\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    Z = cache[\"Z\"]\n",
    "    # your code here\n",
    "    relu_val = np.zeros(Z.shape)\n",
    "    indixes = np.where(Z > 0)\n",
    "    relu_val[indixes] = 1\n",
    "    \n",
    "    dZ = np.multiply(relu_val,dZ)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f852b01b4c73d0b30c98a2ec928f88b",
     "grade": true,
     "grade_id": "test_case2_relu_der",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "dA_tst = np.array([[0,2],[1,1]])\n",
    "cache_tst = {}\n",
    "cache_tst['Z'] = np.array([[-1,2],[1,-2]])\n",
    "npt.assert_array_equal(relu_der(dA_tst,cache_tst),np.array([[0,2],[1,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd03aee42ab92a4b63da967f3759c88e",
     "grade": false,
     "grade_id": "cell-ff93df0fc4bbc430",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Linear activation and its derivative (repeated from Assignment 2)\n",
    "\n",
    "There is no activation involved here. It is an identity function. \n",
    "\\begin{equation*}\n",
    "\\text{Linear}(Z) = Z\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "695ee8c0246dcc69a45cdcc2b32e07cb",
     "grade": false,
     "grade_id": "cell-5c19d5fd5d97fb3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear(Z):\n",
    "    '''\n",
    "    Computes linear activation of Z\n",
    "    This function is implemented for completeness\n",
    "        \n",
    "    Inputs: \n",
    "        Z: numpy.ndarray (n, m) which represent 'm' samples each of 'n' dimension\n",
    "        \n",
    "    Outputs: \n",
    "        A: where A = Linear(Z) is a numpy.ndarray (n, m) representing 'm' samples each of 'n' dimension\n",
    "        cache: a dictionary with {\"Z\", Z}   \n",
    "    '''\n",
    "    A = Z\n",
    "    cache = {}\n",
    "    cache[\"Z\"] = Z\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def linear_der(dA, cache):\n",
    "    '''\n",
    "    Computes derivative of linear activation\n",
    "    This function is implemented for completeness\n",
    "    \n",
    "    Inputs: \n",
    "        dA: derivative from the subsequent layer of dimension (n, m). \n",
    "            dA is multiplied elementwise with the gradient of Linear(.)\n",
    "        cache: dictionary with {\"Z\", Z}, where Z was the input \n",
    "            to the activation layer during forward propagation\n",
    "        \n",
    "    Outputs: \n",
    "        dZ: the derivative of dimension (n,m). It is the elementwise \n",
    "            product of the derivative of Linear(.) and dA\n",
    "    '''      \n",
    "    dZ = np.array(dA, copy=True)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce35f78ff70d66940badc942b348ebf3",
     "grade": false,
     "grade_id": "cell-076c0de6c87fa8af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Softmax Activation and Cross-entropy Loss Function (repeated from Assignment 2:  0 points)\n",
    "\n",
    "The softmax activation is computed on the outputs from the last layer and the output label with the maximum probablity is predicted as class label. The softmax function can also be refered as normalized exponential function which takes a vector of $n$ real numbers as input, and normalizes it into a probability distribution consisting of $n$ probabilities proportional to the exponentials of the input numbers.\n",
    "\n",
    "The input to the softmax function is the $(n \\times m)$ matrix, $ Z = [ z^{(1)} , z^{(2)}, \\ldots, z^{(m)} ] $, where $z^{(i)}$ is the $i^{th}$ sample of $n$ dimensions. We estimate the softmax for each of the samples $1$ to $m$. The softmax activation for sample $z^{(i)}$ is $a^{(i)} = \\text{softmax}(z^{(i)})$, where the components of $a^{(i)}$ are,\n",
    "\\begin{equation}\n",
    "a_k{(i)} = \\frac{\\text{exp}(z^{(i)}_k)}{\\sum_{k = 1}^{n}\\text{exp}(z^{(i)}_k)} \\qquad \\text{for} \\quad 1\\leq k\\leq n\n",
    "\\end{equation}\n",
    "\n",
    "The output of the softmax is $ A = [ a^{(1)} , a^{(2)} .... a^{(m)} ]$, where $a^{(i)} = [a^{(i)}_1,a^{(i)}_2, \\ldots, a^{(i)}_n]^\\top$.  In order to avoid floating point overflow, we subtract a constant from all the input components of $z^{(i)}$ before calculating the softmax. This constant is $z_{max}$, where, $z_{max} = \\text{max}(z_1,z_2,...z_n)$. The activation is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "a_k{(i)} = \\frac{\\text{exp}(z^{(i)}_k- z_{max})}{\\sum_{k = 1}^{n}\\text{exp}(z^{(i)}_k - z_{max})} \\qquad \\text{for} \\quad 1\\leq k\\leq n\n",
    "\\end{equation}\n",
    "\n",
    "If the output of softmax is given by $A$ and the ground truth is given by $Y = [ y^{(1)} , y^{(2)}, \\ldots, y^{(m)}]$, the cross entropy loss between the predictions $A$ and groundtruth labels $Y$ is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "Loss(A,Y) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^{n}I \\{ y^i = k \\} \\text{log}a_k^i\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $I$ is the identity function given by \n",
    "\n",
    "\\begin{equation}\n",
    "I\\{\\text{condition}\\} = 1, \\quad \\text{if condition = True}\\\\\n",
    "I\\{\\text{condition}\\} = 0, \\quad \\text{if condition = False}\\\\\n",
    "\\end{equation}\n",
    "Hint: use [numpy.exp](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)\n",
    "numpy.max,\n",
    "[numpy.sum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html)\n",
    "[numpy.log](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)\n",
    "Also refer to use of 'keepdims' and 'axis' parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96ce0af0b0c8a2cbe0176be9a65aaf55",
     "grade": false,
     "grade_id": "test_case3_softmax_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss(Z, Y=np.array([])):\n",
    "    '''\n",
    "    Computes the softmax activation of the inputs Z\n",
    "    Estimates the cross entropy loss\n",
    "\n",
    "    Inputs: \n",
    "        Z: numpy.ndarray (n, m)\n",
    "        Y: numpy.ndarray (1, m) of labels\n",
    "            when y=[] loss is set to []\n",
    "    \n",
    "    Outputs:\n",
    "        A: numpy.ndarray (n, m) of softmax activations\n",
    "        cache: a dictionary to store the activations which will be used later to estimate derivatives\n",
    "        loss: cost of prediction\n",
    "    '''\n",
    "    \n",
    "    # your code here\n",
    "    Zmax = np.max(Z, axis=0)\n",
    "    e = np.exp(Z-Zmax)\n",
    "    t = e.sum(axis = 0)\n",
    "    A = e/t\n",
    "    if Y.size > 0:\n",
    "        n,m = A.shape\n",
    "        r_Y = Y.reshape(-1).astype(int)\n",
    "        one_hot = np.eye(n)[r_Y].T\n",
    "        loss = np.sum(-np.multiply(one_hot,np.log(A)))/m\n",
    "    else:\n",
    "        loss=[]\n",
    "    cache = {}\n",
    "    cache[\"A\"] = A\n",
    "    return A, cache, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba99e7e708852e0c35b5b6f3665d239d",
     "grade": true,
     "grade_id": "test_case3_softmax",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#test cases for softmax_cross_entropy_loss\n",
    "np.random.seed(1)\n",
    "Z_t = np.random.randn(3,4)\n",
    "Y_t = np.array([[1,0,1,2]])\n",
    "A_t = np.array([[0.57495949, 0.38148818, 0.05547572, 0.36516899],\n",
    "       [0.26917503, 0.07040735, 0.53857622, 0.49875847],\n",
    "       [0.15586548, 0.54810447, 0.40594805, 0.13607254]])\n",
    "\n",
    "A_est, cache_est, loss_est = softmax_cross_entropy_loss(Z_t, Y_t)\n",
    "npt.assert_almost_equal(loss_est,1.2223655548779273,decimal=5)\n",
    "npt.assert_array_almost_equal(A_est,A_t,decimal=5)\n",
    "npt.assert_array_almost_equal(cache_est['A'],A_t,decimal=5)\n",
    "\n",
    "# hidden test cases follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf8515684585cafb58a9d922e3a8720b",
     "grade": false,
     "grade_id": "cell-5151a9f9720ee789",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Derivative of the softmax_cross_entropy_loss(.) (repeated from Assignment 2   -   0 points)\n",
    "\n",
    "We discused in the lecture that it is easier to directly estimate $dZ$ which is $\\frac{dL}{dZ}$, where $Z$ is the input to the *softmax_cross_entropy_loss($Z$)* function. \n",
    "\n",
    "Let $Z$ be the $(n\\times m)$ dimension input and $Y$ be the $(1,m)$ groundtruth labels. If $A$ is the $(n\\times m)$ matrix of softmax activations of $Z$, the derivative $dZ$ is given by, \n",
    "\n",
    "\\begin{equation}\n",
    "dZ =\\frac{1}{m} (A -\\bar{Y})\n",
    "\\end{equation}\n",
    "\n",
    "where, $\\bar{Y}$ is the one-hot representation of $Y$. \n",
    "\n",
    "One-hot encoding is a binary representation of the discrete class labels. For example, let $y^{(i)}\\in\\{0,1,2\\}$ for a 3-category problem. Assume there are $m=4$ data points. In this case $Z$ will be a $3 \\times 4$ matrix. Let the categories of the 4 data points be $Y=[1,0,1,2]$. The one hot representation is given by, \n",
    "\\begin{equation}\n",
    "\\bar{Y} = \n",
    "    \\begin{bmatrix}\n",
    "    0 ~ 1 ~ 0 ~ 0\\\\\n",
    "    1 ~ 0 ~ 1 ~ 0\\\\\n",
    "    0 ~ 0 ~ 0 ~ 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "where, the one-hot encoding for label $y^{(1)} = 1$ is $\\bar{y}^{(1)} = [0, 1, 0]^\\top$. Similarly, the one-hot encoding for $y^{(4)} = 2$ is $\\bar{y}^{(4)} = [0, 0, 1]^\\top$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e131cd252e51deebb53d920f975a4ac5",
     "grade": false,
     "grade_id": "test_case4_softmax_der_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss_der(Y, cache):\n",
    "    '''\n",
    "    Computes the derivative of the softmax activation and cross entropy loss\n",
    "\n",
    "    Inputs: \n",
    "        Y: numpy.ndarray (1, m) of labels\n",
    "        cache: a dictionary with cached activations A of size (n,m)\n",
    "\n",
    "    Outputs:\n",
    "        dZ: derivative dL/dZ - a numpy.ndarray of dimensions (n, m) \n",
    "    '''\n",
    "    A = cache[\"A\"]\n",
    "    # your code here\n",
    "    n,m = A.shape\n",
    "    r_Y = Y.reshape(-1).astype(int)\n",
    "    one_hot = np.eye(n)[r_Y].T\n",
    "    dZ = (1/m)*(A-one_hot)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddb4d9c5e4005c7f4a06cac3537c2c61",
     "grade": true,
     "grade_id": "test_case4_softmax_der",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#test cases for softmax_cross_entropy_loss_der\n",
    "np.random.seed(1)\n",
    "Z_t = np.random.randn(3,4)\n",
    "Y_t = np.array([[1,0,1,2]])\n",
    "A_t = np.array([[0.57495949, 0.38148818, 0.05547572, 0.36516899],\n",
    "       [0.26917503, 0.07040735, 0.53857622, 0.49875847],\n",
    "       [0.15586548, 0.54810447, 0.40594805, 0.13607254]])\n",
    "cache_t={}\n",
    "cache_t['A'] = A_t\n",
    "dZ_t = np.array([[ 0.14373987, -0.15462795,  0.01386893,  0.09129225],\n",
    "       [-0.18270624,  0.01760184, -0.11535594,  0.12468962],\n",
    "       [ 0.03896637,  0.13702612,  0.10148701, -0.21598186]])\n",
    "\n",
    "dZ_est = softmax_cross_entropy_loss_der(Y_t, cache_t)\n",
    "npt.assert_almost_equal(dZ_est,dZ_t,decimal=5)\n",
    "\n",
    "# hidden test cases follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout forward (10 points)\n",
    "\n",
    "The dropout layer is introduced to improve regularization by reducing overfitting. The layer will zero out some of the activations in the input based on the 'drop_prob' value. Dropout is only appiled in 'train' mode and not in 'test' mode. In the 'test' mode the output activations are the same as input activations. \n",
    "We will implement the inverted droput method we discussed in the lecture. We define 'prob_keep' as the percentage of activations remaining after dropout, if drop_out = 0.3, then prob_keep = 0.7, i.e., 70% of the activations are retained after dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b4627af274670c942f2376fe2bac46f",
     "grade": false,
     "grade_id": "cell-e97aa827e6a861ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dropout(A, drop_prob, mode='train'):\n",
    "        '''\n",
    "        Using the 'inverted dropout' technique to implement dropout regularization.\n",
    "        Inputs:\n",
    "            A: Activation input before dropout is applied - shape is (n,m)\n",
    "            drop_prob: dropout parameter. If drop_prob = 0.3, we drop 30% of the neuron activations\n",
    "            mode: Dropout acts differently in training and testing mode. Hence, mode is a parameter which\n",
    "                takes in only 2 values, 'train' or 'test'\n",
    "\n",
    "        Outputs:\n",
    "            A: Output of shape (n,m), with some values masked out and other values scaled to account for missing values\n",
    "            cache: a tuple which stores the drop_prob, mode and mask for use in backward pass.\n",
    "        '''\n",
    "        # When there is no dropout return the same activation\n",
    "        mask = None\n",
    "        if drop_prob == 0:\n",
    "            cache = (drop_prob, mode, mask)\n",
    "            return A, cache\n",
    "        \n",
    "        # The prob_keep is the percentage of activations remaining after dropout\n",
    "        # if drop_out = 0.3, then prob_keep = 0.7, i.e., 70% of the activations are retained\n",
    "        prob_keep = 1-drop_prob\n",
    "        \n",
    "        # Note: instead of a binary mask implement a scaled mask, where mask is scaled by dividing it \n",
    "        # by the prob_keep for example, if we have input activations of size (3,4), then the mask is \n",
    "        # mask = (np.random.rand(3,4)<prob_keep)/prob_keep\n",
    "        # We perform the scaling by prob_keep here so we don't have to do it specifically during backpropagation \n",
    "        # We then update A by multiplying it element wise with the mask\n",
    "        \n",
    "        if mode == 'train':\n",
    "            # your code here\n",
    "            mask = (np.random.rand(A.shape[0],A.shape[1]) < prob_keep)/prob_keep\n",
    "            A = np.multiply(A,mask)\n",
    "        elif mode != 'test':\n",
    "            raise ValueError(\"Mode value not set correctly, set it to 'train' or 'test'\")\n",
    "        cache = (drop_prob, mode, mask)\n",
    "        return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ec618234b01b33f9d45586881a90fc5",
     "grade": true,
     "grade_id": "cell-186e9ebb0627e412",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "x_t = np.random.rand(3,4)\n",
    "drop_prob_t = 0.3\n",
    "x_est, cache_est = dropout(x_t, drop_prob_t, mode='train')\n",
    "npt.assert_array_almost_equal(x_est,np.array(\n",
    "[[5.95745721e-01, 0.0, 1.63392596e-04, 4.31903675e-01],\n",
    " [2.09651273e-01, 1.31912278e-01, 2.66086016e-01, 4.93658181e-01],\n",
    " [0.0,            0.0,            5.98849306e-01, 9.78885001e-01]]\n",
    "),5)\n",
    "npt.assert_array_almost_equal(cache_est[2], np.array(\n",
    "[[1.42857143, 0.,         1.42857143, 1.42857143],\n",
    " [1.42857143, 1.42857143, 1.42857143, 1.42857143],\n",
    " [0.,         0.,         1.42857143, 1.42857143]]\n",
    "),5)\n",
    "assert cache_est[1]=='train'\n",
    "np.random.seed(1)\n",
    "x_t = np.random.rand(3,4)\n",
    "drop_prob_t = 0.3\n",
    "out_est, cache_est = dropout(x_t, drop_prob_t, mode='test')\n",
    "npt.assert_array_almost_equal(out_est, x_t, 6)\n",
    "assert cache_est[1]=='test'\n",
    "np.random.seed(1)\n",
    "x_t = np.random.rand(3,4)\n",
    "drop_prob_t = 0\n",
    "out_est, cache_est = dropout(x_t, drop_prob_t, mode='train')\n",
    "npt.assert_array_almost_equal(out_est, x_t, 6)\n",
    "assert cache_est[0]==0\n",
    "assert cache_est[1]=='train'\n",
    "assert cache_est[2]==None\n",
    "#hidden tests follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout backward (10 points)\n",
    "\n",
    "In the backward pass, we estimate the derivative w.r.t. the dropout layer. We will need the 'drop_prob', 'mask' and 'mode' which is obtained from the cache saved during forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2672cdb298d304e4a8ddf9a77a195e81",
     "grade": false,
     "grade_id": "cell-34ce98b5a9658fae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dropout_der(dA_in, cache):\n",
    "        '''\n",
    "        Backward pass for the inverted dropout.\n",
    "        Inputs: \n",
    "            dA_in: derivative from the upper layers of dimension (n,m).\n",
    "            cache: tuple containing (drop_out, mode, mask), where drop_out is the probability of drop_out, \n",
    "                if drop_out=0, then the layer does not have any dropout,\n",
    "                mode is either 'train' or 'test' and \n",
    "                mask is a matirx of size (n,m) where 0's indicate masked values\n",
    "        Outputs:\n",
    "            dA_out = derivative of the dropout layer of dimension (n,m)\n",
    "        '''\n",
    "        \n",
    "        dA_out = None\n",
    "        drop_out, mode, mask = cache\n",
    "        # If there is no dropout return the same derivative from the previous layer\n",
    "        if not drop_out:\n",
    "            return dA_in\n",
    "        \n",
    "        # if mode is 'train' dA_out is dA_in multiplied element wise by mask\n",
    "        # if mode is 'test' dA_out is same as dA_in\n",
    "        # your code here\n",
    "        if mode == 'train':\n",
    "            dA_out = np.multiply(dA_in,mask)\n",
    "        if mode == 'test':\n",
    "            dA_out = dA_in\n",
    "        return dA_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4268ef2b0bc6d72e605c81879721fa9a",
     "grade": true,
     "grade_id": "cell-f8ed32ff4bf0bd9a",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "dA_in_t = np.random.rand(4,2)\n",
    "mask_t = np.array(\n",
    "[[1.42857143, 1.42857143],\n",
    " [1.42857143, 1.42857143],\n",
    " [1.42857143, 0.        ],\n",
    " [1.42857143, 1.42857143]])\n",
    "mode_t = 'test'\n",
    "drop_prob_t = 0.3\n",
    "cache_t = (drop_prob_t,mode_t, mask_t)\n",
    "dA_out_est = dropout_der(dA_in_t, cache_t)\n",
    "npt.assert_array_almost_equal(dA_out_est, np.array(\n",
    "[\n",
    " [4.17022005e-01, 7.20324493e-01],\n",
    " [1.14374817e-04, 3.02332573e-01],\n",
    " [1.46755891e-01, 9.23385948e-02],\n",
    " [1.86260211e-01, 3.45560727e-01]\n",
    "]),6)\n",
    "\n",
    "mode_t = 'train'\n",
    "cache = (drop_prob_t, mode_t, mask_t)\n",
    "dA_out_est = dropout_der(dA_in_t, cache)\n",
    "npt.assert_almost_equal(dA_out_est,np.array(\n",
    "[\n",
    " [5.95745721e-01, 1.02903499e+00],\n",
    " [1.63392596e-04, 4.31903675e-01],\n",
    " [2.09651273e-01, 0.0           ],\n",
    " [2.66086016e-01, 4.93658181e-01]\n",
    "]),6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a23870ebbde5eeae58d49b8077aed39e",
     "grade": false,
     "grade_id": "cell-8ccc857eca28aa9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Batchnorm forward (15 points)\n",
    "\n",
    "Batchnorm scales the input activations in a minibatch to have a specific mean and variance allowing the training to use larger learning rates to improve training speeds and provide more stability to the training. During training, the input minibatch is first normalized by making it zero mean and scaled to unit variance, i.e., ($0,I$) normalized. The normalized data is then converted to have a mean ($\\beta$) and variance ($\\gamma$), i.e., ($\\beta,\\gamma I$) normalized. Here, $\\beta$ and $\\gamma$ are the parameters for the batchnorm layer which are updated during training using gradient descent. \n",
    "The original batchnorm paper was implemented by applying batchnorm before nonlinear activation. However, batchnorm has been found to be more effective when applied after activation. We will implement this version in Assignment 3. \n",
    "\n",
    "In the lecture, we also discussed implementation of batchnorm during test mode when a single sample may be input for evaluation. We will not be implementing this aspect of batchnorm for the assignment. This implementation will work as designed only when a minibatch of data is presented to the network during evaluation (test mode) and may not work as expected when a single image is input for evaluation (test mode). \n",
    "\n",
    "The batchnorm implementation is tricky, especially the backpropagation. You may use the following source for reference: [Batchnorm backpropagation Tutorial](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html). Note: The tutorial representes data as a $(m,n)$ matrix, whereas we represent data as a $(n,m)$ matrix, where $n$ is feature dimensions and $m$ is number of samples. \n",
    "\n",
    "If you are unable to implement the batchnorm correctly, you can still get the network to work by setting the variable 'bnorm_list = [0,0,...,0,0]. This is a list of binary varibles indicating if batchnorm is used for a layer (0 means no batchorm for the corresponding layer). This variable is used in the 'multi_layer_network(.)' function when initalizing the network. \n",
    "Although the follwing testcases may fail, the network can still work without batchnorm and you can get partial credit. \n",
    "\n",
    "The variables you save into the cache is your choice. The tescase only tests for the normalized output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bc0845e4a2333e69c031fe554c4e921",
     "grade": false,
     "grade_id": "batchnorm_forward_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def batchnorm(A, beta, gamma):\n",
    "    '''\n",
    "    Batchnorm normalizes the input A to mean beta and standard deviation gamma\n",
    "    \n",
    "    Inputs: \n",
    "        A: Activation input after activation - shape is (n,m), m samples where each sample x is (n,1)\n",
    "        beta: mean vector which will be the center of the data after batchnorm - shape is (n,1)\n",
    "        gamma: standard deviation vector which will be scale of the data after batchnorm - shape (n,1)\n",
    "        \n",
    "    Outputs: \n",
    "        Anorm: Normalized version of input A - shape (n,m)\n",
    "        cache: Dictionary of the elements that are necessary for backpropagation\n",
    "    '''\n",
    "    \n",
    "    # When there is no batch norm for a layer, the beta and gamma will be empty arrays\n",
    "    if beta.size == 0 or gamma.size == 0:\n",
    "        cache = {}\n",
    "        return A, cache\n",
    "    # epsilon value used for scaling during normalization to avoid divide by zero. \n",
    "    # don't change this value - the test case will fail if you change this value\n",
    "    epsilon = 1e-5\n",
    "    # your code here\n",
    "    n, m = A.shape\n",
    "    mu = 1./m * np.sum(A, axis=1, keepdims=True)\n",
    "    Amu = A - mu\n",
    "    sq = Amu**2\n",
    "    var = 1./m * np.sum(sq, axis=1, keepdims=True)\n",
    "    sqrtvar = np.sqrt(var + epsilon)\n",
    "    ivar = 1./sqrtvar\n",
    "    Ahat = Amu * ivar \n",
    "    \n",
    "    gammaA = gamma * Ahat\n",
    "    Anorm = gammaA + beta\n",
    "    cache = {}\n",
    "    cache[\"Ahat\"] = Ahat\n",
    "    cache[\"gamma\"] = gamma\n",
    "    cache[\"Amu\"] = Amu\n",
    "    cache[\"ivar\"] = ivar\n",
    "    cache[\"sqrtvar\"] = sqrtvar\n",
    "    cache[\"var\"] = var\n",
    "    cache[\"epsilon\"] = epsilon\n",
    "    return Anorm, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b455a20620ddc1f598a35bf733a004b",
     "grade": true,
     "grade_id": "batchnorm_forward",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "A_t = np.array([[1., 2.],[3., 4.],[5., 6]])\n",
    "beta_t = np.array([[1.], [2.], [3.]])\n",
    "gamma_t = np.array([[4.], [5.], [6.]])\n",
    "Anorm_est, cache_est = batchnorm(A_t, beta_t, gamma_t)\n",
    "npt.assert_array_almost_equal(Anorm_est,np.array(\n",
    "[\n",
    " [-2.99992,  4.99992],\n",
    " [-2.9999,   6.9999 ],\n",
    " [-2.99988,  8.99988]\n",
    "]),5)\n",
    "\n",
    "# There are hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "235204f5e72f927c473528f0a1155a10",
     "grade": false,
     "grade_id": "cell-a8cf50ba49b92a32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Batchnorm backward (15 points)\n",
    "\n",
    "The forward propagation for batchnorm is relatively straightfoward to implement. For the backward propagation to worrk, you will need to save a set of variables in the cache during the forward propagation. The variables in your cache are your choice. The testcase only tests for the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb73a49523de475c649d69105c08ceac",
     "grade": false,
     "grade_id": "batchnorm_backward_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def batchnorm_der(dA_in, cache):\n",
    "    '''\n",
    "    Derivative of the batchnorm\n",
    "    Inputs: \n",
    "        dA_in: derivative from the upper layers of dimension (n,m).\n",
    "        cache: Dictionary of the elements that are necessary for backpropagation\n",
    "    Outputs:\n",
    "        dA_out: derivative of the batchnorm layer of dimension (n,m)\n",
    "        dbeta: derivative of beta - shape (n,1)\n",
    "        dgamma: derivative of gamma - shape (n,1)\n",
    "    '''\n",
    "    # When the cache is empty, it indicates there was no batchnorm for the layer\n",
    "    if not cache:\n",
    "        dbeta = []\n",
    "        dgamma = []\n",
    "        return dA_in, dbeta, dgamma\n",
    "    \n",
    "    # your code here\n",
    "    n, m = dA_in.shape\n",
    "    dbeta = np.sum(dA_in, axis=1, keepdims=True)\n",
    "    dgamma = np.sum(dA_in*cache[\"Ahat\"], axis=1,keepdims=True)\n",
    "    dAhat = dA_in * cache[\"gamma\"]\n",
    "    divar = np.sum(dAhat*cache[\"Amu\"], axis=1,keepdims=True)\n",
    "    dAmu1 = dAhat * cache[\"ivar\"]\n",
    "    dsqrtvar = -1./(cache[\"sqrtvar\"]**2)*divar\n",
    "    dvar = 0.5*1./np.sqrt(cache[\"var\"]+cache[\"epsilon\"])*dsqrtvar\n",
    "    dsq = 1./m * np.ones((n,m))*dvar\n",
    "    dAmu2 = 2*cache[\"Amu\"]*dsq\n",
    "    dA1 = (dAmu1+dAmu2)\n",
    "    dmu = -1 * np.sum(dAmu1+dAmu2, axis=1, keepdims=True)\n",
    "    dA2 = 1./m * np.ones((n,m))*dmu\n",
    "    dA_out = dA1 +dA2\n",
    "    \n",
    "    return dA_out, dbeta, dgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0af813c4cdf435717076d9fc55d03e5e",
     "grade": true,
     "grade_id": "batchnorm_backward",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "A_t = np.array([[1., 2.],[3., 4.],[5., 6]])\n",
    "beta_t = np.array([[1.], [2.], [3.]])\n",
    "gamma_t = np.array([[4.], [5.], [6.]])\n",
    "Anorm_t, cache_t = batchnorm(A_t, beta_t, gamma_t)\n",
    "dA_in_t = np.array(\n",
    "[\n",
    "  [4.,  5.],\n",
    "  [8.,  10.],\n",
    "  [12., 15.]\n",
    "])\n",
    "dA_out_est, dbeta_est, dgamma_est = batchnorm_der(dA_in_t, cache_t)\n",
    "npt.assert_array_almost_equal(dA_out_est,np.array(\n",
    "[\n",
    " [ -0.0001600,    0.0001600],\n",
    " [ -0.0004000,    0.0004000],\n",
    " [ -0.0007200,    0.0007200]\n",
    "]),5)\n",
    "npt.assert_array_almost_equal(dbeta_est,np.array([[9.],[18.],[27.]]),5)\n",
    "npt.assert_array_almost_equal(dgamma_est,np.array([[1.],[2.],[3.]]),4)\n",
    "\n",
    "# There are hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2afffb84ecba8a8f56c26aece3a4998c",
     "grade": false,
     "grade_id": "cell-2d445d2fe1bb530d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Parameter Initialization (5 points)\n",
    "\n",
    "We will define the function to initialize the parameters of the multi-layer neural network.\n",
    "The network parameters will be stored as dictionary elements that can easily be passed as function parameters while calculating gradients during back propogation.\n",
    "\n",
    "The parameters are initialized using Kaiming He Initialization (discussed in the lecture). For example, a layer with weights of dimensions $(n_{out}, n_{in})$, the parameters are initialized as\n",
    "$w = np.random.randn(n_{out},n_{in})*(2./np.sqrt(n_{in}))$ and \n",
    "$b = np.zeros((n_{out},1))$\n",
    "\n",
    "The dimension for weight matrix for layer $(l+1)$ is given by ( Number-of-neurons-in-layer-$(l+1)$   $\\times$   Number-of-neurons-in-layer-$l$ ). The dimension of the bias for for layer $(l+1)$ is (Number-of-neurons-in-layer-$(l+1)$   $\\times$   1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5776d8f512e36381fef5991f897725c0",
     "grade": false,
     "grade_id": "params_initialize_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_network(net_dims, act_list, drop_prob_list):\n",
    "    '''\n",
    "    Initializes the parameters W's and b's of a multi-layer neural network\n",
    "    Adds information about dropout and activations in each layer\n",
    "    \n",
    "    Inputs:\n",
    "        net_dims: List containing the dimensions of the network. The values of the array represent the number of nodes in \n",
    "        each layer. For Example, if a Neural network contains 784 nodes in the input layer, 800 in the first hidden layer, \n",
    "        500 in the secound hidden layer and 10 in the output layer, then net_dims = [784,800,500,10]. \n",
    "        act_list: list of strings indicating the activation for a layer\n",
    "        drop_prob_list: list of dropout probabilities for each layer \n",
    "    \n",
    "    Outputs:\n",
    "        parameters: dictionary of \n",
    "                    {\"numLayers\":..}\n",
    "                    activations, {\"act1\":\"..\", \"act2\":\"..\", ...}\n",
    "                    dropouts, {\"dropout1\": .. , \"dropout2\": .., ...}\n",
    "                    network parameters, {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "            The weights are initialized using Kaiming He et al. Initialization\n",
    "    '''\n",
    "    net_dims_len = len(net_dims)\n",
    "    parameters = {}\n",
    "    parameters['numLayers'] = net_dims_len - 1;\n",
    "    for l in range(net_dims_len-1):\n",
    "        parameters[\"act\"+str(l+1)] = act_list[l]\n",
    "        parameters[\"dropout\"+str(l+1)] = drop_prob_list[l]\n",
    "        # Note: Use He et al. Initialization to initialize W and set bias to 0's\n",
    "        # parameters[\"W\"+str(l+1)] = \n",
    "        # parameters[\"b\"+str(l+1)] =\n",
    "        # your code here\n",
    "        parameters[\"W\"+str(l+1)] = np.random.randn(net_dims[l+1], net_dims[l]) * (2./np.sqrt(net_dims[l]))\n",
    "        parameters[\"b\"+str(l+1)] = np.zeros((net_dims[l+1],1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94e0b6b2c740ec0a066d1d5b06937bfd",
     "grade": true,
     "grade_id": "params_initialize",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Test \n",
    "np.random.seed(1)\n",
    "net_dims_t = [3,4,1]\n",
    "act_list_t = ['relu', 'linear']\n",
    "drop_prob_list = [0.3,0.5]\n",
    "parameters_est = initialize_network(net_dims_t, act_list_t, drop_prob_list)\n",
    "npt.assert_array_almost_equal(parameters_est['W1'], [\n",
    " [ 1.87563247, -0.70639546, -0.60988021],\n",
    " [-1.23895745,  0.99928666, -2.65758797],\n",
    " [ 2.01473508, -0.87896602,  0.36839462],\n",
    " [-0.28794811,  1.68829682, -2.37884559]\n",
    "],6)\n",
    "assert parameters_est['W1'].shape == (4,3)\n",
    "assert parameters_est['W2'].shape == (1,4)\n",
    "assert parameters_est['b1'].shape == (4,1)\n",
    "assert parameters_est['b2'].shape == (1,1)\n",
    "assert parameters_est['b1'].all() == 0\n",
    "assert parameters_est['b2'].all() == 0\n",
    "assert parameters_est['act1'] == 'relu'\n",
    "assert parameters_est['act2'] == 'linear'\n",
    "assert parameters_est['dropout1'] == 0.3\n",
    "assert parameters_est['dropout2'] == 0.5\n",
    "\n",
    "# There are hidden tests\n",
    "# HIDDEN TESTS ###\n",
    "net_dims_t = [60,44,6]\n",
    "act_list_t = ['linear', 'relu']\n",
    "drop_prob_list = [0.5,0]\n",
    "parameters_est = initialize_network(net_dims_t, act_list_t, drop_prob_list)\n",
    "assert parameters_est['W1'].shape == (44,60)\n",
    "assert parameters_est['W2'].shape == (6,44)\n",
    "assert parameters_est['b1'].shape == (44,1)\n",
    "assert parameters_est['b2'].shape == (6,1)\n",
    "assert parameters_est['b1'].all() == 0\n",
    "assert parameters_est['b2'].all() == 0\n",
    "assert parameters_est['act1'] == 'linear'\n",
    "assert parameters_est['act2'] == 'relu'\n",
    "assert parameters_est['dropout1'] == 0.5\n",
    "assert parameters_est['dropout2'] == 0\n",
    "# HIDDEN TESTS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e02e8f835aa4d3015ff23b01f6fc4a1e",
     "grade": false,
     "grade_id": "cell-ffe90db2e8c100f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Adam (momentum) Parameters - Velocity and Gradient-Squares Initialization (5 points)\n",
    "\n",
    "We will optmize using Adam (momentum). This requires velocity parameters $V$ and Gradient-Squares parameters $G$. Here is a quick recap of Adam optmization, \n",
    "\n",
    "\\begin{equation} \n",
    "V_{t+1} = \\beta V_{t} +(1-\\beta)\\nabla J(\\theta_t)\\\\ \n",
    "G_{t+1} = \\beta_2 G_{t} +(1-\\beta_2)\\nabla J(\\theta_t)^2\\\\ \n",
    "\\theta_{t+1} =\\theta_{t} -\\frac{\\alpha}{\\sqrt{G_{t+1}+\\epsilon}}V_{t+1}, \\quad \\theta \\in \\{ W,b \\} \n",
    "\\end{equation}\n",
    "\n",
    "Parameters $V$ are the momentum velocity parameters and parameters $G$ are the Gradient-Squares. \n",
    "$\\nabla J(\\theta)$ is the gradient term $dW$ or $db$, and $\\nabla  J(\\theta)^2$ is the element wise square of the gradient. \n",
    "$\\alpha$ is the step_size for gradient descent. It is has been estimated by decaying the 'learning_rate' based on 'decay_rat'e and 'epoch' number. $\\beta$, $\\beta_2$ and $\\epsilon$ are constants which we will set up later.\n",
    "\n",
    "Each of the parameters $W$'s and $b$'s for all the layers will have their corresponding velocity ($V$) and Gradient-Squares ($G$) parameters. The following function will initialize $V$ and $G$ to zeros with the same size as the corresponding parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e57ee4eb4685673bb0629cc87fcf2475",
     "grade": false,
     "grade_id": "cell-dd58518f843ee3ec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters, apply_momentum=True):\n",
    "    '''\n",
    "    The function will add Adam momentum parameters, Velocity and Gradient-Squares \n",
    "    to the parameters for each of the W's and b's \n",
    "    \n",
    "    Inputs: \n",
    "        parameters: dictionary containing, \n",
    "                    {\"numLayers\":..}\n",
    "                    activations, {\"act1\":\"..\", \"act2\":\"..\", ...}\n",
    "                    dropouts, {\"dropout1\": .. , \"dropout2\": .., ...}\n",
    "                    network parameters, {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "        apply_momentum: boolean on whether to apply momentum\n",
    "        \n",
    "    Outputs:\n",
    "        parameters: dictionary that has been updated to include velocity and Gradient-Squares. It now contains,\n",
    "                    {\"numLayers\":..}\n",
    "                    activations, {\"act1\":\"..\", \"act2\":\"..\", ...}\n",
    "                    dropouts, {\"dropout1\": .. , \"dropout2\": .., ...}\n",
    "                    {\"apply_momentum\":..}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "    '''\n",
    "    \n",
    "    L = parameters['numLayers'] \n",
    "    parameters['apply_momentum'] = apply_momentum\n",
    "    \n",
    "    # Initialize Velocity and the Gradient-Squares to zeros the same size as the corresponding parameters W's abd b's\n",
    "    for l in range(L):\n",
    "        if apply_momentum:\n",
    "            # Hint: Velocity parameters are represented as VdW and Vdb\n",
    "            #      Gradient-Squares are represented as GdW and Gdb\n",
    "            # You can use np.zeros_like(.) to initilaize them 0's the same size as corresponding parameters W and b\n",
    "            # parameters[\"VdW\" + str(l+1)] = \n",
    "            # parameters[\"Vdb\" + str(l+1)] =\n",
    "            # parameters[\"GdW\" + str(l+1)] =\n",
    "            # parameters[\"Gdb\" + str(l+1)] =\n",
    "            # your code here\n",
    "            \n",
    "            parameters[\"VdW\" + str(l+1)] = np.zeros(parameters[\"W\"+str(l+1)].shape)\n",
    "            parameters[\"Vdb\" + str(l+1)] = np.zeros(parameters[\"b\"+str(l+1)].shape)\n",
    "            parameters[\"GdW\" + str(l+1)] = np.zeros(parameters[\"W\"+str(l+1)].shape)\n",
    "            parameters[\"Gdb\" + str(l+1)] = np.zeros(parameters[\"b\"+str(l+1)].shape)\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10c8698ec51d04aec1f7901ea186c98f",
     "grade": true,
     "grade_id": "cell-5346e9cc7d26c1e3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Test \n",
    "net_dims_t = [5,4,1]\n",
    "act_list_t = ['relu', 'linear']\n",
    "drop_prob_list = [0.3,0.5]\n",
    "parameters_t = initialize_network(net_dims_t, act_list_t, drop_prob_list)\n",
    "parameters_t = initialize_velocity(parameters_t)\n",
    "assert parameters_t['VdW1'].shape == (4,5)\n",
    "assert parameters_t['VdW1'].all() == 0\n",
    "assert parameters_t['VdW2'].shape == (1,4)\n",
    "assert parameters_t['VdW2'].all() == 0\n",
    "assert parameters_t['Vdb1'].shape == (4,1)\n",
    "assert parameters_t['Vdb2'].shape == (1,1) \n",
    "assert parameters_t['Vdb1'].all() == 0\n",
    "assert parameters_t['Vdb2'].all() == 0\n",
    "assert parameters_t['GdW1'].shape == (4,5)\n",
    "assert parameters_t['GdW1'].all() == 0\n",
    "assert parameters_t['GdW2'].shape == (1,4)\n",
    "assert parameters_t['GdW2'].all() == 0\n",
    "assert parameters_t['Gdb1'].shape == (4,1)\n",
    "assert parameters_t['Gdb2'].shape == (1,1)\n",
    "assert parameters_t['Gdb1'].all() == 0\n",
    "assert parameters_t['Gdb2'].all() == 0\n",
    "assert parameters_t['apply_momentum'] == True\n",
    "# There are hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "372ead70fa94bbbb5eefe274e3222488",
     "grade": false,
     "grade_id": "cell-e8ad576b87cea2fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Batchnorm Parameters and corresponding Velocity and Gradient-Squares Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6f2e5619b0754c69df03c22395830cd",
     "grade": false,
     "grade_id": "cell-74f99853a41130d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_bnorm_params(parameters, bnorm_list, apply_momentum):\n",
    "    '''\n",
    "    The function will add batchnorm parameters beta's and gamma's and their corresponding\n",
    "    Velocity and Gradient-Squares to the parameters dictionary\n",
    "    \n",
    "    Inputs: \n",
    "        parameters: dictionary that contains,\n",
    "                    {\"numLayers\":..}\n",
    "                    activations, {\"act1\":\"..\", \"act2\":\"..\", ...}\n",
    "                    dropouts, {\"dropout1\": .. , \"dropout2\": .., ...}\n",
    "                    {\"apply_momentum\":..}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "        bnorm_list: binary list indicating if batchnorm should be implemented for a layer\n",
    "        apply_momentum: boolean on whether to apply momentum\n",
    "        \n",
    "    Outputs:\n",
    "        parameters: dictionary that has been updated to include batchnorm parameters, beta, gamma \n",
    "                    and their corresponding momentum parameters. It now contains,\n",
    "                    {\"numLayers\":..}\n",
    "                    activations, {\"act1\":\"..\", \"act2\":\"..\", ...}\n",
    "                    dropouts, {\"dropout1\": .. , \"dropout2\": .., ...}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    {\"bnorm_list\":..}\n",
    "                    batchnorm parameters, {\"bnorm_beta1\":[..],\"bnorm_gamma1\":[..],\"bnorm_beta2\":[..],\"bnorm_gamma2\":[..],...}\n",
    "                    batchnorm velocity parameters, {\"Vbnorm_beta1\":[..],\"Vbnorm_gamma1\":[..],\"Vbnorm_beta2\":[..],\"Vbnorm_gamma2\":[..],...}\n",
    "                    batchnorm Gradient-Square parameters, {\"Gbnorm_beta1\":[..],\"Gbnorm_gamma1\":[..],\"Gbnorm_beta2\":[..],\"Gbnorm_gamma2\":[..],...}\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "    '''\n",
    "    \n",
    "    L = parameters['numLayers']\n",
    "    parameters['bnorm_list'] = bnorm_list\n",
    "    \n",
    "    # Initialize batchnorm parameters for the hidden layers only. \n",
    "    # Each hidden layer will have a dictionary of parameters, beta and gamma based on the dimensions of the hidden layer. \n",
    "    for l in range(L):\n",
    "        if bnorm_list[l]:\n",
    "            n = parameters[\"W\" + str(l+1)].shape[0]\n",
    "            parameters['bnorm_beta'+str(l+1)] = np.random.randn(n,1)\n",
    "            parameters['bnorm_gamma'+str(l+1)] = np.random.randn(n,1)\n",
    "            if apply_momentum:\n",
    "                parameters['Vbnorm_beta'+str(l+1)] = np.zeros((n,1))\n",
    "                parameters['Gbnorm_beta'+str(l+1)] = np.zeros((n,1))\n",
    "                parameters['Vbnorm_gamma'+str(l+1)] = np.zeros((n,1))\n",
    "                parameters['Gbnorm_gamma'+str(l+1)] = np.zeros((n,1))\n",
    "        else:\n",
    "            parameters['bnorm_beta'+str(l+1)] = np.asarray([])\n",
    "            parameters['Vbnorm_beta'+str(l+1)] = np.asarray([])\n",
    "            parameters['Gbnorm_beta'+str(l+1)] = np.asarray([])\n",
    "            parameters['bnorm_gamma'+str(l+1)] = np.asarray([])\n",
    "            parameters['Vbnorm_gamma'+str(l+1)] = np.asarray([])\n",
    "            parameters['Gbnorm_gamma'+str(l+1)] = np.asarray([])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba2dbfdf3f3b183229914f723e1417d7",
     "grade": false,
     "grade_id": "cell-36236b2868ecda30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Forward Propagation Through a Single Layer (repeated from Assignment 2 - 0 points)\n",
    "\n",
    "If the vectorized input to any layer of neural network is $A\\_prev$ and the parameters of the layer are given by $(W,b)$, the output of the layer (before the activation is):\n",
    "\\begin{equation}\n",
    "Z = W.A\\_prev + b\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed4865aef2f388d832326ef6c4160c95",
     "grade": false,
     "grade_id": "forward_single_layer_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    '''\n",
    "    Input A_prev propagates through the layer \n",
    "    Z = WA + b is the output of this layer. \n",
    "\n",
    "    Inputs: \n",
    "        A_prev: numpy.ndarray (n,m) the input to the layer\n",
    "        W: numpy.ndarray (n_out, n) the weights of the layer\n",
    "        b: numpy.ndarray (n_out, 1) the bias of the layer\n",
    "\n",
    "    Outputs:\n",
    "        Z: where Z = W.A_prev + b, where Z is the numpy.ndarray (n_out, m) dimensions\n",
    "        cache: a dictionary containing the inputs A\n",
    "    '''\n",
    "    # your code here\n",
    "    \n",
    "    Z = np.dot(W,A_prev)+b\n",
    "    cache = {}\n",
    "    cache[\"A\"] = A_prev\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff99bfd982a9c8c02c5df742e8a1c251",
     "grade": true,
     "grade_id": "forward_single_layer",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Hidden test cases follow\n",
    "np.random.seed(1)\n",
    "n1 = 3\n",
    "m1 = 4\n",
    "A_prev_t = np.random.randn(n1,m1)\n",
    "W_t = np.random.randn(n1, n1)\n",
    "b_t = np.random.randn(n1, 1)\n",
    "Z_est, cache_est = linear_forward(A_prev_t, W_t, b_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a78556ac91473852bcf96924e78854ad",
     "grade": false,
     "grade_id": "cell-3c463f4362ff9844",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Forward Propagation Through a Layer (linear $\\rightarrow$ activation $\\rightarrow$ batchnorm $\\rightarrow$ dropout)\n",
    "\n",
    "The input to the layer propagates through the layer in the order linear $\\rightarrow$ activation $\\rightarrow$ batchnorm $\\rightarrow$ dropout saving different cache along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07631dfdb8eefbc842508a3536dffaaa",
     "grade": false,
     "grade_id": "cell-f70a31ac49d9bb4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def layer_forward(A_prev, W, b, activation, drop_prob, bnorm_beta, bnorm_gamma, mode):\n",
    "    '''\n",
    "    Input A_prev propagates through the layer followed by activation, batchnorm and dropout\n",
    "\n",
    "    Inputs: \n",
    "        A_prev: numpy.ndarray (n,m) the input to the layer\n",
    "        W: numpy.ndarray (n_out, n) the weights of the layer\n",
    "        b: numpy.ndarray (n_out, 1) the bias of the layer\n",
    "        activation: is the string that specifies the activation function\n",
    "        drop_prob: dropout parameter. If drop_prob = 0.3, we drop 30% of the neuron activations\n",
    "        bnorm_beta: batchnorm beta \n",
    "        bnorm_gamma: batchnorm gamma\n",
    "        mode: 'train' or 'test' Dropout acts differently in training and testing mode. Hence, mode is a parameter which\n",
    "                takes in only 2 values, 'train' or 'test'\n",
    "\n",
    "    Outputs:\n",
    "        A: = g(Z), where Z = WA + b, where Z is the numpy.ndarray (n_out, m) dimensions\n",
    "        g is the activation function\n",
    "        cache: a dictionary containing the cache from the linear propagation, activation, bacthnorm and dropout\n",
    "        to be used for derivative\n",
    "    '''\n",
    "    \n",
    "    Z, lin_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"relu\":\n",
    "        A, act_cache = relu(Z)\n",
    "    elif activation == \"linear\":\n",
    "        A, act_cache = linear(Z)\n",
    "    \n",
    "    A, bnorm_cache = batchnorm(A, bnorm_beta, bnorm_gamma)\n",
    "    A, drop_cache = dropout(A, drop_prob, mode)\n",
    "    cache = {}\n",
    "    cache[\"lin_cache\"] = lin_cache\n",
    "    cache[\"act_cache\"] = act_cache\n",
    "    cache[\"bnorm_cache\"] = bnorm_cache\n",
    "    cache[\"drop_cache\"] = drop_cache\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "611df0adfac42ead393f088d11e8826e",
     "grade": false,
     "grade_id": "cell-243ec2c02ff1834c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multi-Layers Forward Propagation\n",
    "\n",
    "Starting with the input 'A0' and the first layer of the network, we will propgate A0 through every layer using the output of the previous layer as input to the next layer. we will gather the caches from every layer in a list and use it later for backpropagation. We will use the 'layer_forward(.)' function to get the output and caches for a layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd87521073a2f0a0c07338d938566eb7",
     "grade": false,
     "grade_id": "cell-40ada61852849730",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multi_layer_forward(A0, parameters, mode):\n",
    "    '''\n",
    "    Forward propgation through the layers of the network\n",
    "\n",
    "    Inputs: \n",
    "        A0: numpy.ndarray (n,m) with n features and m samples\n",
    "        parameters: dictionary of network parameters {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
    "        mode: 'train' or 'test' Dropout acts differently in training and testing mode. Hence, mode is a parameter which\n",
    "                takes in only 2 values, 'train' or 'test' \n",
    "    \n",
    "    Outputs:\n",
    "        AL: numpy.ndarray (c,m)  - outputs of the last fully connected layer before softmax\n",
    "            where c is number of categories and m is number of samples\n",
    "        caches: a list of caches from every layer after forward propagation\n",
    "    '''\n",
    "    \n",
    "    L = parameters['numLayers']\n",
    "    A = A0\n",
    "    caches = []\n",
    "    for l in range(L):\n",
    "        A, cache = layer_forward(A, parameters[\"W\"+str(l+1)], parameters[\"b\"+str(l+1)], \\\n",
    "                                 parameters[\"act\"+str(l+1)], parameters[\"dropout\"+str(l+1)], \\\n",
    "                                 parameters['bnorm_beta'+str(l+1)], parameters['bnorm_gamma'+str(l+1)], mode)\n",
    "        caches.append(cache)\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "136f3914445b836e7df4bba3238ce216",
     "grade": false,
     "grade_id": "cell-5281900868c84d39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Backward Propagagtion for the Linear Computation of a Layer (repeated from Assignment 2 - 0 points)\n",
    "\n",
    "Consider the linear layer $Z = W.A\\_prev + b$. We would like to estimate the gradients $\\frac{dL}{dW}$ - represented as $dW$, $\\frac{dL}{db}$ - represented as $db$ and $\\frac{dL}{dA\\_prev}$ - represented as $dA\\_prev$. \n",
    "The input to estimate these derivatives is $\\frac{dL}{dZ}$ - represented as $dZ$. The derivatives are given by, \n",
    "\n",
    "\\begin{equation}\n",
    "dA\\_prev = W^T dZ\\\\\n",
    "dW = dZ A^T\\\\\n",
    "db = \\sum_{i=1}^{m} dZ^{(i)}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "where $dZ = [dz^{(1)},dz^{(2)}, \\ldots, dz^{(m)}]$ is $(n \\times m)$ matrix of derivatives. \n",
    "The figure below represents a case fo binary cassification where $dZ$ is of dimensions $(1 \\times m)$. The example can be extended to $(n\\times m)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf41a82263726e6035c2585266317b26",
     "grade": false,
     "grade_id": "linear_backward_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache, W, b):\n",
    "    '''\n",
    "    Backward prpagation through the linear layer\n",
    "\n",
    "    Inputs:\n",
    "        dZ: numpy.ndarray (n,m) derivative dL/dz \n",
    "        cache: a dictionary containing the inputs A, for the linear layer\n",
    "            where Z = WA + b,    \n",
    "            Z is (n,m); W is (n,p); A is (p,m); b is (n,1)\n",
    "        W: numpy.ndarray (n,p)\n",
    "        b: numpy.ndarray (n,1)\n",
    "\n",
    "    Outputs:\n",
    "        dA_prev: numpy.ndarray (p,m) the derivative to the previous layer\n",
    "        dW: numpy.ndarray (n,p) the gradient of W \n",
    "        db: numpy.ndarray (n,1) the gradient of b\n",
    "    '''\n",
    "    \n",
    "    A = cache[\"A\"]\n",
    "    # your code here\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    dW = np.dot(dZ,A.T)\n",
    "    db = np.sum(dZ, axis=1, keepdims = True)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d739ea390f5d7e02b6798ce19d59dabc",
     "grade": true,
     "grade_id": "linear_backward",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Hidden test cases follow\n",
    "np.random.seed(1)\n",
    "n1 = 3\n",
    "m1 = 4\n",
    "p1 = 5\n",
    "dZ_t = np.random.randn(n1,m1)\n",
    "A_t = np.random.randn(p1,m1)\n",
    "cache_t = {}\n",
    "cache_t['A'] = A_t\n",
    "W_t = np.random.randn(n1,p1)\n",
    "b_t = np.random.randn(n1,1)\n",
    "\n",
    "dA_prev_est, dW_est, db_est = linear_backward(dZ_t, cache_t, W_t, b_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e7ae2a750bd9690d211025d37278304",
     "grade": false,
     "grade_id": "cell-f57dc4108dd56c38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Back Propagation Through a Layer (dropout $\\rightarrow$ batchnorm $\\rightarrow$ activation $\\rightarrow$ linear)\n",
    "\n",
    "We will define the backpropagation for a layer. We will use the backpropagation for the dropout, followed by backpropagation for batchnorm, backpropagation of activation and backpropagation of a linear layer, in that order. This is the reverse order to the forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21b514c5a427c236a4954e4465ad2fc4",
     "grade": false,
     "grade_id": "cell-9b7cfa89255f3e03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def layer_backward(dA, cache, W, b, activation):\n",
    "    '''\n",
    "    Backward propagation through the activation and linear layer\n",
    "\n",
    "    Inputs:\n",
    "        dA: numpy.ndarray (n,m) the derivative to the previous layer\n",
    "        cache: dictionary containing the linear_cache and the activation_cache\n",
    "        W: numpy.ndarray (n,p)\n",
    "        b: numpy.ndarray (n,1)\n",
    "        activation: activation of the layer, 'relu' or 'linear'\n",
    "    \n",
    "    Outputs:\n",
    "        dA_prev: numpy.ndarray (p,m) the derivative to the previous layer\n",
    "        dW: numpy.ndarray (n,p) the gradient of W \n",
    "        db: numpy.ndarray (n,1) the gradient of b\n",
    "        dbnorm_beta: numpy.ndarray (n,1) derivative of beta for the batchnorm layer\n",
    "        dbnorm_gamma: numpy.ndarray (n,1) derivative of gamma for the batchnorm layer\n",
    "    '''\n",
    "\n",
    "    lin_cache = cache[\"lin_cache\"]\n",
    "    act_cache = cache[\"act_cache\"]\n",
    "    drop_cache = cache[\"drop_cache\"]\n",
    "    bnorm_cache = cache[\"bnorm_cache\"]\n",
    "    \n",
    "    dA = dropout_der(dA, drop_cache)\n",
    "    dA, dbnorm_beta, dbnorm_gamma = batchnorm_der(dA, cache[\"bnorm_cache\"])\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_der(dA, act_cache)\n",
    "    elif activation == \"linear\":\n",
    "        dZ = linear_der(dA, act_cache)\n",
    "        \n",
    "    dA_prev, dW, db = linear_backward(dZ, lin_cache, W, b)\n",
    "    return dA_prev, dW, db, dbnorm_beta, dbnorm_gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63dd4a8b2ce39e28960758bcc6393bfc",
     "grade": false,
     "grade_id": "cell-72a9dc0cb265dc90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multi-layers Back Propagation\n",
    "\n",
    "We have defined the required functions to handle back propagation for a single layer. Now we will stack the layers together and perform back propagation on the entire network starting with the final layer. We will need teh caches stored during forward propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3c530dd1f62e5cee0dc1c3626ab8832",
     "grade": false,
     "grade_id": "cell-8d2141e7c67dafa5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multi_layer_backward(dAL, caches, parameters):\n",
    "    '''\n",
    "    Back propgation through the layers of the network (except softmax cross entropy)\n",
    "    softmax_cross_entropy can be handled separately\n",
    "\n",
    "    Inputs: \n",
    "        dAL: numpy.ndarray (n,m) derivatives from the softmax_cross_entropy layer\n",
    "        caches: a dictionary of associated caches of parameters and network inputs\n",
    "        parameters: dictionary of network parameters {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
    "\n",
    "    Outputs:\n",
    "        gradients: dictionary of gradient of network parameters \n",
    "            {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...\\\n",
    "            \"dbnorm_beta1\":[..],\"dbnorm_gamma1\":[..],\"dbnorm_beta2\":[..],\"dbnorm_gamma2\":[..],...}\n",
    "    '''\n",
    "\n",
    "    L = len(caches) \n",
    "    gradients = {}\n",
    "    dA = dAL\n",
    "    activation = \"linear\"\n",
    "    for l in reversed(range(L)):\n",
    "        dA, gradients[\"dW\"+str(l+1)], gradients[\"db\"+str(l+1)], \\\n",
    "        gradients[\"dbnorm_beta\"+str(l+1)], gradients[\"dbnorm_gamma\"+str(l+1)] \\\n",
    "                    = layer_backward(dA, caches[l], parameters[\"W\"+str(l+1)],\\\n",
    "                                     parameters[\"b\"+str(l+1)],parameters[\"act\"+str(l+1)])\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6f9f7593d557173167d6a84f6f678cb",
     "grade": false,
     "grade_id": "cell-4cb88cabf2344894",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Parameter Update Using Adam (momentum) (15 points)\n",
    "\n",
    "The parameter gradients $(dW,db)$ calculated during back propagation are used to update the values of the network parameters using Adam optmization which is the momentum technique we discussed in the lecture.\n",
    "\n",
    "\\begin{equation} \n",
    "V_{t+1} = \\beta V_{t} +(1-\\beta)\\nabla J(\\theta_t)\\\\ \n",
    "G_{t+1} = \\beta_2 G_{t} +(1-\\beta_2)\\nabla J(\\theta_t)^2\\\\ \n",
    "\\theta_{t+1} =\\theta_{t} -\\frac{\\alpha}{\\sqrt{G_{t+1}+\\epsilon}}V_{t+1}, \\quad \\theta \\in \\{ W,b \\} \n",
    "\\end{equation}\n",
    "\n",
    "Parameters $V$ are the momentum velocity parameters and parameters $G$ are the Gradient-Squares. \n",
    "$\\nabla J(\\theta)$ is the gradient term $dW$ or $db$, and $\\nabla  J(\\theta)^2$ is the element wise square of the gradient. \n",
    "$\\alpha$ is the step_size for gradient descent. It is has been estimated by decaying the learning_rate based on decay_rate and epoch number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b17fe2a2e6d731a24e28a6235d1c389",
     "grade": false,
     "grade_id": "update_momentum_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum_Adam(parameters, gradients, alpha, beta=0.9, beta2=0.99, eps=1e-8):\n",
    "    '''\n",
    "    Updates the network parameters with gradient descent\n",
    "\n",
    "    Inputs:\n",
    "        parameters: dictionary of \n",
    "                    network parameters, {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    batchnorm parameters, {\"bnorm_beta1\":[..],\"bnorm_gamma1\":[..],\"bnorm_beta2\":[..],\"bnorm_gamma2\":[..],...}\n",
    "                    batchnorm velocity parameters, {\"Vbnorm_beta1\":[..],\"Vbnorm_gamma1\":[..],\"Vbnorm_beta2\":[..],\"Vbnorm_gamma2\":[..],...}\n",
    "                    batchnorm Gradient-Square parameters, {\"Gbnorm_beta1\":[..],\"Gbnorm_gamma1\":[..],\"Gbnorm_beta2\":[..],\"Gbnorm_gamma2\":[..],...}\n",
    "                    and other parameters \n",
    "                    :\n",
    "                    :\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "        gradients: dictionary of gradient of network parameters \n",
    "                   {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...}\n",
    "        alpha: stepsize for the gradient descent\n",
    "        beta: beta parameter for momentum (same as beta1 in Adam)\n",
    "        beta2: beta2 parameter for Adam\n",
    "        eps: epsilon parameter for Adam\n",
    "        \n",
    "    Outputs: \n",
    "        parameters: updated dictionary of \n",
    "                    network parameters, {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    batchnorm parameters, {\"bnorm_beta1\":[..],\"bnorm_gamma1\":[..],\"bnorm_beta2\":[..],\"bnorm_gamma2\":[..],...}\n",
    "                    batchnorm velocity parameters, {\"Vbnorm_beta1\":[..],\"Vbnorm_gamma1\":[..],\"Vbnorm_beta2\":[..],\"Vbnorm_gamma2\":[..],...}\n",
    "                    batchnorm Gradient-Square parameters, {\"Gbnorm_beta1\":[..],\"Gbnorm_gamma1\":[..],\"Gbnorm_beta2\":[..],\"Gbnorm_gamma2\":[..],...}\n",
    "                    and other parameters \n",
    "                    :\n",
    "                    :\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "             \n",
    "    '''\n",
    "    L = parameters['numLayers']\n",
    "    apply_momentum = parameters['apply_momentum']\n",
    "    bnorm_list = parameters['bnorm_list']\n",
    "    \n",
    "    for l in range(L):\n",
    "        if apply_momentum:\n",
    "            # Apply Adam momentum to parameters W's and b's. \n",
    "            # You will need to update the Velocity parameters VdW's and Vdb's\n",
    "            # parameters[\"VdW\" + str(l+1)] = \n",
    "            # parameters[\"Vdb\" + str(l+1)] =\n",
    "            # You will need to update the Gradient-Squares parameters GdW's and Gdb's\n",
    "            # parameters[\"GdW\" + str(l+1)] = \n",
    "            # parameters[\"Gdb\" + str(l+1)] =\n",
    "            # You will need to update the parameters W's and b's\n",
    "            # parameters[\"W\" + str(l+1)] = \n",
    "            # parameters[\"b\" + str(l+1)] = \n",
    "            # your code here\n",
    "            parameters[\"VdW\" + str(l+1)] = (beta*parameters[\"VdW\"+str(l+1)])+((1-beta)*gradients[\"dW\"+str(l+1)])\n",
    "            parameters[\"Vdb\" + str(l+1)] = (beta*parameters[\"Vdb\"+str(l+1)])+((1-beta)*gradients[\"db\"+str(l+1)])\n",
    "\n",
    "\n",
    "            parameters[\"GdW\" + str(l+1)] = (beta2*parameters[\"GdW\"+str(l+1)])+((1-beta2)*np.multiply(gradients[\"dW\"+str(l+1)],gradients[\"dW\"+str(l+1)]))\n",
    "            parameters[\"Gdb\" + str(l+1)] =(beta2*parameters[\"Gdb\"+str(l+1)])+((1-beta2)*np.multiply(gradients[\"db\"+str(l+1)],gradients[\"db\"+str(l+1)]))\n",
    "\n",
    "            parameters[\"W\"+str(l+1)] -= np.multiply((alpha/np.sqrt(parameters[\"GdW\"+str(l+1)]+eps)),parameters[\"VdW\"+str(l+1)])\n",
    "            parameters[\"b\"+str(l+1)] -= np.multiply((alpha/np.sqrt(parameters[\"Gdb\"+str(l+1)]+eps)),parameters[\"Vdb\"+str(l+1)])\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # When no momentum is required apply regular gradient descent\n",
    "            parameters[\"W\"+str(l+1)] -= alpha * gradients[\"dW\"+str(l+1)]\n",
    "            parameters[\"b\"+str(l+1)] -= alpha * gradients[\"db\"+str(l+1)]\n",
    "        \n",
    "        # The Adam momentum for batch norm parameters has been implemented below\n",
    "        if apply_momentum and bnorm_list[l]:\n",
    "            parameters['Vbnorm_beta'+str(l+1)] = beta*parameters['Vbnorm_beta'+str(l+1)] + \\\n",
    "                                                    (1 - beta)*gradients[\"dbnorm_beta\"+str(l+1)]\n",
    "            parameters['Vbnorm_gamma'+str(l+1)] = beta*parameters['Vbnorm_gamma'+str(l+1)] + \\\n",
    "                                                    (1 - beta)*gradients[\"dbnorm_gamma\"+str(l+1)]\n",
    "            parameters['Gbnorm_beta'+str(l+1)] = beta2*parameters['Gbnorm_beta'+str(l+1)] + \\\n",
    "                                                    (1 - beta2)*(gradients[\"dbnorm_beta\"+str(l+1)]**2)\n",
    "            parameters['Gbnorm_gamma'+str(l+1)] = beta2*parameters['Gbnorm_gamma'+str(l+1)] + \\\n",
    "                                                    (1 - beta2)*(gradients[\"dbnorm_gamma\"+str(l+1)]**2)\n",
    "            parameters['bnorm_beta' + str(l+1)] = parameters['bnorm_beta' + str(l+1)] \\\n",
    "                        - alpha*parameters['Vbnorm_beta'+str(l+1)]/np.sqrt(parameters['Gbnorm_beta'+str(l+1)] + eps)\n",
    "            parameters['bnorm_gamma' + str(l+1)] = parameters['bnorm_gamma' + str(l+1)] \\\n",
    "                        - alpha*parameters['Vbnorm_gamma'+str(l+1)]/np.sqrt(parameters['Gbnorm_gamma'+str(l+1)] + eps)\n",
    "        elif bnorm_list[l]:\n",
    "            parameters['bnorm_beta' + str(l+1)] -= alpha * gradients[\"dbnorm_beta\"+str(l+1)]\n",
    "            parameters['bnorm_gamma' + str(l+1)] -= alpha * gradients[\"dbnorm_beta\"+str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f97558d40b2681227c5ad08fd26599c4",
     "grade": true,
     "grade_id": "update_momentum",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Testing without apply_momentum\n",
    "apply_momentum_t=True\n",
    "np.random.seed(1)\n",
    "n1 = 3\n",
    "m1 = 4\n",
    "p1 = 2\n",
    "A0_t = np.random.randn(n1,m1)\n",
    "Y_t = np.array([[1., 0., 0., 1.]])\n",
    "net_dims_t = [n1,m1,p1]\n",
    "act_list_t = ['relu', 'linear']\n",
    "drop_prob_list_t = [0.3,0]\n",
    "parameters_t = initialize_network(net_dims_t, act_list_t, drop_prob_list_t)\n",
    "parameters_t = initialize_velocity(parameters_t, apply_momentum_t)\n",
    "bnorm_list_t = [0,0]\n",
    "parameters_t = initialize_bnorm_params(parameters_t, bnorm_list_t, apply_momentum_t)\n",
    "A_t, caches_t = multi_layer_forward(A0_t, parameters_t, 'train')\n",
    "AL_t,softmax_cache_t,_ = softmax_cross_entropy_loss(A_t,Y_t)\n",
    "dAL_t = softmax_cross_entropy_loss_der(Y_t, softmax_cache_t)\n",
    "gradients_t = multi_layer_backward(dAL_t, caches_t, parameters_t)\n",
    "parameters_t = update_parameters_with_momentum_Adam(parameters_t, gradients_t, alpha=1)\n",
    "parameters_t = update_parameters_with_momentum_Adam(parameters_t, gradients_t, alpha=1)\n",
    "npt.assert_array_almost_equal(parameters_t['W1'],np.array(\n",
    "      [[-2.7191573 , -2.79033829,  3.6559832 ],\n",
    "       [ 1.07680635,  2.14776979,  1.3330437 ],\n",
    "       [-2.29708863, -1.67003859, -3.38885276],\n",
    "       [ 3.66847147, -1.30571348, -1.76653199]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['b1'],np.array([[ 2.34687006],[-2.3468659 ],[-2.34495502],[-2.34646538]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['W2'],np.array(\n",
    "      [[ 3.247725  ,  1.66314062, -2.46646577,  1.41104969],\n",
    "       [-2.61475713, -1.81651301,  1.65191479, -2.74357265]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['b2'],np.array([[2.34685986],[-2.34685986]]),decimal=6)\n",
    "\n",
    "npt.assert_array_almost_equal(parameters_t['VdW1'],np.array(\n",
    "      [[ 0.04959329,  0.08954598, -0.02339858],\n",
    "       [-0.03635384, -0.13950981, -0.01346142],\n",
    "       [ 0.00538899,  0.00279682,  0.00033368],\n",
    "       [-0.01183912,  0.01798966,  0.01645148]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['Vdb1'],np.array([[-0.08461959],[0.06005603],[0.0039684 ],[0.00860207]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['GdW1'],np.array(\n",
    "      [[1.35578790e-03, 4.42016104e-03, 3.01803982e-04],\n",
    "       [7.28528402e-04, 1.07289049e-02, 9.98913468e-05],\n",
    "       [1.60088503e-05, 4.31196318e-06, 6.13777575e-08],\n",
    "       [7.72653956e-05, 1.78398694e-04, 1.49195616e-04]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['Gdb1'],np.array([[3.94718747e-03],[1.98819556e-03],[8.68115211e-06],[4.07898679e-05]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['VdW2'],np.array(\n",
    "      [[-0.07598473, -0.07214759,  0.00302548, -0.02342229],\n",
    "       [ 0.07598473,  0.07214759, -0.00302548,  0.02342229]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['Vdb2'],np.array([[-0.0457975],[0.0457975]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['GdW2'],np.array(\n",
    "      [[3.18272027e-03, 2.86938965e-03, 5.04586721e-06, 3.02415905e-04],\n",
    "       [3.18272027e-03, 2.86938965e-03, 5.04586721e-06, 3.02415905e-04]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['Gdb2'],np.array([[0.00115619],[0.00115619]]),decimal=6)\n",
    "\n",
    "# Testing without momentum\n",
    "np.random.seed(1)\n",
    "apply_momentum_t=False\n",
    "n1 = 2\n",
    "m1 = 3\n",
    "p1 = 2\n",
    "A0_t = np.random.randn(n1,m1)\n",
    "Y_t = np.array([[1., 0., 0.]])\n",
    "net_dims_t = [n1,m1,p1]\n",
    "act_list_t = ['relu', 'linear']\n",
    "drop_prob_list_t = [0.3,0]\n",
    "parameters_t = initialize_network(net_dims_t, act_list_t, drop_prob_list_t)\n",
    "parameters_t = initialize_velocity(parameters_t, apply_momentum_t)\n",
    "bnorm_list_t = [0,0]\n",
    "parameters_t = initialize_bnorm_params(parameters_t, bnorm_list_t, apply_momentum_t)\n",
    "A_t, caches_t = multi_layer_forward(A0_t, parameters_t, 'train')\n",
    "AL_t,softmax_cache_t,_ = softmax_cross_entropy_loss(A_t,Y_t)\n",
    "dAL_t = softmax_cross_entropy_loss_der(Y_t, softmax_cache_t)\n",
    "gradients_t = multi_layer_backward(dAL_t, caches_t, parameters_t)\n",
    "parameters_t = update_parameters_with_momentum_Adam(parameters_t, gradients_t, alpha=1)\n",
    "parameters_t = update_parameters_with_momentum_Adam(parameters_t, gradients_t, alpha=1)\n",
    "npt.assert_array_almost_equal(parameters_t['W1'],np.array(\n",
    "      [[ 2.38556532, -1.43370309],\n",
    "       [ 0.82922072, -0.60237324],\n",
    "       [-1.52567143, -0.53983959]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['b1'],np.array([[0.1551979 ],[0.23272841],[-2.21221693]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['W2'],np.array(\n",
    "      [[-0.16928131, -1.50183323, -4.86682037],\n",
    "       [-1.47305905,  0.85926252,  5.16232096]]),decimal=6)\n",
    "npt.assert_array_almost_equal(parameters_t['b2'],np.array([[-0.21232129],[0.21232129]]),decimal=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd7d5ca20b9bd5d6aa52f678918cd624",
     "grade": false,
     "grade_id": "cell-76abe4d415a1f55e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multilayer Neural Network (10 points)\n",
    "\n",
    "Let us now assemble all the components of the neural network together and define a complete training loop for a Multi-layer Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0f5f0363eb77f5dd63edc993f5e1608",
     "grade": false,
     "grade_id": "multi_layer_network_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multi_layer_network(X, Y, net_dims, act_list, drop_prob_list, bnorm_list, num_epochs=3, \n",
    "                        batch_size=64, learning_rate=0.2, decay_rate=0.01, apply_momentum=True, log=True, log_step=200):\n",
    "    \n",
    "    '''\n",
    "    Creates the multilayer network and trains the network\n",
    "\n",
    "    Inputs:\n",
    "        X: numpy.ndarray (n,m) of training data\n",
    "        Y: numpy.ndarray (1,m) of training data labels\n",
    "        net_dims: tuple of layer dimensions\n",
    "        act_list: list of strings indicating the activations for each layer\n",
    "        drop_prob_list: list of dropout probabilities for each layer \n",
    "        bnorm_list: binary list indicating presence or absence of batchnorm for each layer\n",
    "        num_epochs: num of epochs to train\n",
    "        batch_size: batch size for training\n",
    "        learning_rate: learning rate for gradient descent\n",
    "        decay_rate: rate of learning rate decay\n",
    "        apply_momentum: boolean whether to apply momentum or not\n",
    "        log: boolean whether to print training progression \n",
    "        log_step: prints training progress every log_step iterations\n",
    "    \n",
    "    Outputs:\n",
    "        costs: list of costs (or loss) over training\n",
    "        parameters: dictionary of \n",
    "                    network parameters, {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    batchnorm parameters, {\"bnorm_beta1\":[..],\"bnorm_gamma1\":[..],\"bnorm_beta2\":[..],\"bnorm_gamma2\":[..],...}\n",
    "                    batchnorm velocity parameters, {\"Vbnorm_beta1\":[..],\"Vbnorm_gamma1\":[..],\"Vbnorm_beta2\":[..],\"Vbnorm_gamma2\":[..],...}\n",
    "                    batchnorm Gradient-Square parameters, {\"Gbnorm_beta1\":[..],\"Gbnorm_gamma1\":[..],\"Gbnorm_beta2\":[..],\"Gbnorm_gamma2\":[..],...}\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "    '''\n",
    "    mode = 'train'\n",
    "    n, m = X.shape\n",
    "    parameters = initialize_network(net_dims, act_list, drop_prob_list)\n",
    "    parameters = initialize_velocity(parameters, apply_momentum)\n",
    "    parameters = initialize_bnorm_params(parameters, bnorm_list, apply_momentum)\n",
    "    costs = []\n",
    "    itr = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # estimate stepsize alpha using decay_rate on learning rate using epoch number\n",
    "        alpha = learning_rate*(1/(1+decay_rate*epoch))\n",
    "        if log:\n",
    "            print('------- Epoch {} -------'.format(epoch+1))\n",
    "        for ii in range((m - 1) // batch_size + 1):\n",
    "            Xb = X[:, ii*batch_size : (ii+1)*batch_size]\n",
    "            Yb = Y[:, ii*batch_size : (ii+1)*batch_size]\n",
    "            A0 = Xb\n",
    "        \n",
    "            ## Forward Propagation\n",
    "            # Step 1: Input 'A0', 'parameters' and 'mode' into the network \n",
    "            #         using multi_layer_forward() and calculate output of last layer 'A' (before softmax) \n",
    "            #         and obtain cached activations as 'caches'\n",
    "            # Step 2: Input 'A' and groundtruth labels 'Yb' to softmax_cros_entropy_loss(.) and estimate\n",
    "            #         activations 'AL', 'softmax_cache' and 'cost'\n",
    "\n",
    "            ## Back Propagation\n",
    "            # Step 3: Estimate gradient 'dAL' with softmax_cros_entropy_loss_der(.) using groundtruth \n",
    "            #         labels 'Yb' and 'softmax_cache' \n",
    "            # Step 4: Estimate 'gradients' with multi_layer_backward(.) using 'dAL', 'caches' and 'parameters' \n",
    "            # Step 5: Estimate updated 'parameters' with update_parameters_with_momentum_Adam(.) \n",
    "            #         using 'parameters', 'gradients' and 'alpha'\n",
    "            #         Note: Use the same variable 'parameters' as input and output to the update_parameters(.) function\n",
    "        \n",
    "            # your code here\n",
    "            \n",
    "            A, caches = multi_layer_forward(A0, parameters, mode)\n",
    "\n",
    "            AL, softmax_cache, cost = softmax_cross_entropy_loss(A, Yb)\n",
    "\n",
    "            dAL = softmax_cross_entropy_loss_der(Yb, softmax_cache)\n",
    "            gradients = multi_layer_backward(dAL, caches, parameters)\n",
    "            parameters = update_parameters_with_momentum_Adam(parameters, gradients, alpha)\n",
    "            if itr % log_step == 0:\n",
    "                costs.append(cost)\n",
    "                if log:\n",
    "                    print(\"Cost at iteration %i is: %.05f, learning rate: %.05f\" %(itr, cost, alpha))\n",
    "            itr+=1\n",
    "    \n",
    "    return costs, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5b30052c20c8d00b5334fb1161a632d",
     "grade": true,
     "grade_id": "cell-multi_layer_network",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n1 = 3\n",
    "m1 = 6\n",
    "p1 = 3\n",
    "A0_t = np.random.randn(n1,m1)\n",
    "Y_t = np.array([[1., 0., 2., 0, 1., 2.]])\n",
    "net_dims_t = [n1,m1,p1]\n",
    "act_list_t = ['relu', 'linear']\n",
    "drop_prob_list_t = [0.3,0]\n",
    "bnorm_list_t = [0,0]\n",
    "num_epochs_t = 1\n",
    "batch_size_t = 2\n",
    "learning_rate_t = 1e-1\n",
    "decay_rate_t = 1\n",
    "apply_momentum_t = True\n",
    "costs_est, parameters_est = multi_layer_network(A0_t, Y_t, net_dims_t, act_list_t, drop_prob_list_t, bnorm_list_t, \\\n",
    "                                        num_epochs=num_epochs_t, batch_size=batch_size_t, learning_rate=learning_rate_t, \\\n",
    "                                        decay_rate=decay_rate_t, apply_momentum=apply_momentum_t, log=False, log_step=1)\n",
    "npt.assert_array_almost_equal(costs_est,np.array([0.8291501476754569, 3.1183677832322285, 4.988510889921268]),decimal=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8258b52466398b97d07c4fac54a14329",
     "grade": false,
     "grade_id": "cell-66defde04ecba045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Prediction (5 points)\n",
    "\n",
    "This is the evaluation function which will predict the labels for a minibatch of inputs samples\n",
    "We will perform forward propagation through the entire network and determine the class predictions for the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c648f2b41d8b9602fac153cb151e1127",
     "grade": false,
     "grade_id": "classify_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def classify(X, parameters, mode='test'):\n",
    "    '''\n",
    "    Network prediction for inputs X\n",
    "\n",
    "    Inputs: \n",
    "        X: numpy.ndarray (n,m) with n features and m samples\n",
    "        parameters: dictionary of network parameters \n",
    "            {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "        drop_prob_list: list of dropout probabilities for each layer \n",
    "        mode: 'train' or 'test' Dropout acts differently in training and testing mode.\n",
    "        \n",
    "    Outputs:\n",
    "        YPred: numpy.ndarray (1,m) of predictions\n",
    "    '''\n",
    "    # Using multi_layer_forward(.) Forward propagate input 'X' with 'parameters' and mode to \n",
    "    #        obtain the final activation 'A'\n",
    "    # Using 'softmax_cross_entropy loss(.)', obtain softmax activation 'AL' with input 'A' from step 1\n",
    "    # Estimate 'YPred' as the 'argmax' of softmax activation from step-2. These are the label predictions \n",
    "    # Note: the shape of 'YPred' should be (1,m), where m is the number of samples\n",
    "    \n",
    "    # your code here\n",
    "    A, caches = multi_layer_forward(X, parameters, mode)\n",
    "    AL, cache, loss = softmax_cross_entropy_loss(A)\n",
    "    YPred = np.argmax(AL, axis=0)\n",
    "    \n",
    "    return YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b18208760c4d806ebc58b7aba782f149",
     "grade": true,
     "grade_id": "classify",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Hidden test cases follow\n",
    "np.random.seed(1)\n",
    "n1 = 3\n",
    "m1 = 4\n",
    "p1 = 2\n",
    "X_t = np.random.randn(n1,m1)\n",
    "net_dims_t = [n1,m1,p1]\n",
    "act_list_t = ['relu', 'linear']\n",
    "drop_prob_list = [0.3,0]\n",
    "parameters_t = initialize_network(net_dims_t, act_list_t, drop_prob_list)\n",
    "bnorm_list_t = [0,0]\n",
    "parameters_t = initialize_bnorm_params(parameters_t, bnorm_list_t, False)\n",
    "#parameters_t = {'W1':W1_t, 'b1':b1_t, 'W2':W2_t, 'b2':b2_t, 'numLayers':2, 'act1':'relu', 'act2':'linear'}\n",
    "YPred_est = classify(X_t, parameters_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a8d461d5399d9e219915a9fb4744e46",
     "grade": false,
     "grade_id": "cell-f44aae42add8fd84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training (10 points)\n",
    "\n",
    "We will now intialize a neural network with 3 hidden layers whose dimensions are 100, 100 and 64. \n",
    "Since the input samples are of dimension 28 $\\times$ 28, the input layer will be of dimension 784. The output dimension is 10 since we have a 10 category classification. \n",
    "We will train the model and compute its accuracy on both training and test sets and plot the training cost (or loss) against the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e838550fae62d69a1a9965e4800dfd35",
     "grade": false,
     "grade_id": "test_acc_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# You should be able to get a train accuracy of >90% and a test accuracy >85% \n",
    "# The settings below gave >95% train accuracy and >90% test accuracy \n",
    "\n",
    "# Feel free to adjust the values and explore how the network behaves\n",
    "net_dims = [784, 100, 100, 64, 10] # This network has 4 layers\n",
    "#784 is for image dimensions\n",
    "#10 is for number of categories \n",
    "#100 and 64 are arbitrary\n",
    "\n",
    "# list of dropout probabilities for each layer\n",
    "# The length of the list is equal to the number of layers\n",
    "# Note: Has to be same length as net_dims. 0 indicates no dropout\n",
    "drop_prob_list = [0, 0, 0, 0]\n",
    "\n",
    "# binary list indicating if batchnorm should be implemented for a layer\n",
    "# The length of the list is equal to the number of layers\n",
    "# 1 indicates bathnorm and 0 indicates no batchnorm\n",
    "# If your implementation of batchnorm is incorrect, then set bnorm_list = [0,0,0,0]\n",
    "bnorm_list = [1,1,1,1]\n",
    "assert(len(bnorm_list) == len(net_dims)-1)\n",
    "\n",
    "# list of strings indicating the activation for a layer\n",
    "# The length of the list is equal to the number of layers\n",
    "# The last layer is usually a linear before the softmax\n",
    "act_list = ['relu', 'relu', 'relu', 'linear']\n",
    "assert(len(act_list) == len(net_dims)-1)\n",
    "    \n",
    "# initialize learning rate, decay_rate and num_iterations \n",
    "num_epochs = 3\n",
    "batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "decay_rate = 1\n",
    "apply_momentum = True\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "print(\"Network dimensions are:\" + str(net_dims))\n",
    "print('Dropout= [{}], Batch Size = {}, lr = {}, decay rate = {}'\\\n",
    "      .format(drop_prob_list,batch_size,learning_rate,decay_rate)) \n",
    "\n",
    "# getting the subset dataset from MNIST\n",
    "trX, trY, tsX, tsY = get_mnist()\n",
    "# We need to reshape the data everytime to match the format (d,m), where d is dimensions (784) and m is number of samples\n",
    "trX = trX.reshape(-1, 28*28).T\n",
    "trY = trY.reshape(1, -1)\n",
    "tsX = tsX.reshape(-1, 28*28).T\n",
    "tsY = tsY.reshape(1, -1)\n",
    "\n",
    "costs, parameters = multi_layer_network(trX, trY, net_dims, act_list, drop_prob_list, bnorm_list, \\\n",
    "                                        num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate, \\\n",
    "                                        decay_rate=decay_rate, apply_momentum=apply_momentum, log=True)\n",
    "            \n",
    "\n",
    "# compute the accuracy for training set and testing set\n",
    "train_Pred = classify(trX, parameters)\n",
    "\n",
    "test_Pred = classify(tsX, parameters)\n",
    "\n",
    "# Estimate the training accuracy 'trAcc' comparing train_Pred and trY \n",
    "# Estimate the testing accuracy 'teAcc' comparing test_Pred and tsY\n",
    "# your code here\n",
    "trAcc = np.mean(np.double(train_Pred == trY)) * 100\n",
    "teAcc = np.mean(np.double(test_Pred == tsY)) * 100\n",
    "\n",
    "print(\"Accuracy for training set is {0:0.3f} %\".format(trAcc))\n",
    "print(\"Accuracy for testing set is {0:0.3f} %\".format(teAcc))\n",
    "\n",
    "plt.plot(range(len(costs)),costs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following set up gives an accuracy of > 96% for both test and train. \n",
    "# Feel free to change the settings to get the best accuracy \n",
    "np.random.seed(1)\n",
    "\n",
    "net_dims = [784, 100, 100, 10] \n",
    "drop_prob_list = [0, 0, 0]\n",
    "act_list = ['relu', 'relu', 'linear']\n",
    "    \n",
    "# initialize learning rate, decay_rate and num_iterations \n",
    "num_epochs = 3\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "decay_rate = 0.1\n",
    "apply_momentum = True\n",
    "\n",
    "# If your implementation of batchnorm is incorrect, \n",
    "# then set bnorm_list = [0,0,0] below to run the following testcase without batchnorm. \n",
    "# The test case is still expected to pass without batchnorm when your accuracy is above 95%\n",
    "bnorm_list = [1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64e37b89e48e9733c099ea03a0aedf51",
     "grade": true,
     "grade_id": "test_acc",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# getting the subset dataset from MNIST\n",
    "trX, trY, tsX, tsY = get_mnist()\n",
    "# We need to reshape the data everytime to match the format (d,m), where d is dimensions (784) and m is number of samples\n",
    "trX = trX.reshape(-1, 28*28).T\n",
    "trY = trY.reshape(1, -1)\n",
    "tsX = tsX.reshape(-1, 28*28).T\n",
    "tsY = tsY.reshape(1, -1)\n",
    "\n",
    "costs, parameters = multi_layer_network(trX, trY, net_dims, act_list, drop_prob_list, bnorm_list, \\\n",
    "                                        num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate, \\\n",
    "                                    decay_rate=decay_rate, apply_momentum=apply_momentum, log=False)\n",
    "\n",
    "# compute the accuracy for training set and testing set\n",
    "train_Pred = classify(trX, parameters)\n",
    "test_Pred = classify(tsX, parameters)\n",
    "\n",
    "# Contains hidden tests \n",
    "# Should get atleast 95% train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets, svm, metrics\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# digits = datasets.load_digits()\n",
    "# n_samples = len(digits.images)\n",
    "# data = digits.images.reshape((n_samples, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "\n",
    "# net_dims = [64, 100, 100, 10] \n",
    "# drop_prob_list = [0, 0, 0]\n",
    "# act_list = ['relu', 'relu', 'linear']\n",
    "    \n",
    "# # initialize learning rate, decay_rate and num_iterations \n",
    "# num_epochs = 3\n",
    "# batch_size = 64\n",
    "# learning_rate = 1e-3\n",
    "# decay_rate = 0.1\n",
    "# apply_momentum = True\n",
    "\n",
    "# # If your implementation of batchnorm is incorrect, \n",
    "# # then set bnorm_list = [0,0,0] below to run the following testcase without batchnorm. \n",
    "# # The test case is still expected to pass without batchnorm when your accuracy is above 95%\n",
    "# bnorm_list = [1,1,1]\n",
    "# # getting the subset dataset from MNIST\n",
    "# # trX, trY, tsX, tsY = get_mnist()\n",
    "\n",
    "# # We need to reshape the data everytime to match the format (d,m), where d is dimensions (784) and m is number of samples\n",
    "# trX = trX.reshape(-1, 8*8).T\n",
    "# trY = trY.reshape(1, -1)\n",
    "# tsX = tsX.reshape(-1, 8*8).T\n",
    "# tsY = tsY.reshape(1, -1)\n",
    "\n",
    "# costs, parameters = multi_layer_network(trX, trY, net_dims, act_list, drop_prob_list, bnorm_list, \\\n",
    "#                                         num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate, \\\n",
    "#                                         decay_rate=decay_rate, apply_momentum=apply_momentum, log=False)\n",
    "\n",
    "# # compute the accuracy for training set and testing set\n",
    "# train_Pred = classify(trX, parameters)\n",
    "# test_Pred = classify(tsX, parameters)\n",
    "\n",
    "# # Contains hidden tests \n",
    "# # Should get atleast 95% train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network dimensions are:[64, 100, 100, 64, 10]\n",
      "Dropout= [[0, 0, 0, 0]], Batch Size = 64, lr = 0.01, decay rate = 1\n",
      "(1, 898)\n",
      "------- Epoch 1 -------\n",
      "Cost at iteration 0 is: 2.94025, learning rate: 0.01000\n",
      "------- Epoch 2 -------\n",
      "------- Epoch 3 -------\n",
      "Accuracy for training set is 75.947 %\n",
      "Accuracy for testing set is 71.635 %\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"264.675263pt\" version=\"1.1\" viewBox=\"0 0 392.14375 264.675263\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-01T18:06:48.474228</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 264.675263 \nL 392.14375 264.675263 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 227.119013 \nL 384.94375 227.119013 \nL 384.94375 9.679013 \nL 50.14375 9.679013 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc4c3025f27\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"95.798295\" xlink:href=\"#mc4c3025f27\" y=\"227.119013\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −0.04 -->\n      <g transform=\"translate(80.475639 241.71745)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"156.671023\" xlink:href=\"#mc4c3025f27\" y=\"227.119013\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −0.02 -->\n      <g transform=\"translate(141.348366 241.71745)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"217.54375\" xlink:href=\"#mc4c3025f27\" y=\"227.119013\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.00 -->\n      <g transform=\"translate(206.410938 241.71745)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"278.416477\" xlink:href=\"#mc4c3025f27\" y=\"227.119013\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.02 -->\n      <g transform=\"translate(267.283665 241.71745)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.289205\" xlink:href=\"#mc4c3025f27\" y=\"227.119013\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.04 -->\n      <g transform=\"translate(328.156392 241.71745)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Iterations -->\n     <g transform=\"translate(193.730469 255.395575)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"29.492188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"68.701172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"130.224609\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"171.337891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"232.617188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"271.826172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"299.609375\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"360.791016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"424.169922\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"me84fa0228e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#me84fa0228e\" y=\"212.688935\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 2.80 -->\n      <g transform=\"translate(20.878125 216.488154)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#me84fa0228e\" y=\"179.073982\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 2.85 -->\n      <g transform=\"translate(20.878125 182.873201)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#me84fa0228e\" y=\"145.45903\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2.90 -->\n      <g transform=\"translate(20.878125 149.258248)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#me84fa0228e\" y=\"111.844077\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2.95 -->\n      <g transform=\"translate(20.878125 115.643296)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#me84fa0228e\" y=\"78.229124\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 3.00 -->\n      <g transform=\"translate(20.878125 82.028343)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#me84fa0228e\" y=\"44.614171\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 3.05 -->\n      <g transform=\"translate(20.878125 48.41339)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#me84fa0228e\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 3.10 -->\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Loss -->\n     <g transform=\"translate(14.798438 129.3662)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#pc743add6ef)\" d=\"M 217.54375 118.399013 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 227.119013 \nL 50.14375 9.679013 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 227.119013 \nL 384.94375 9.679013 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 227.119013 \nL 384.94375 227.119013 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 9.679013 \nL 384.94375 9.679013 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pc743add6ef\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"9.679013\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# # You should be able to get a train accuracy of >90% and a test accuracy >85% \n",
    "# # The settings below gave >95% train accuracy and >90% test accuracy \n",
    "\n",
    "# # Feel free to adjust the values and explore how the network behaves\n",
    "# net_dims = [64, 100, 100, 64, 10] # This network has 4 layers\n",
    "# #784 is for image dimensions\n",
    "# #10 is for number of categories \n",
    "# #100 and 64 are arbitrary\n",
    "\n",
    "# # list of dropout probabilities for each layer\n",
    "# # The length of the list is equal to the number of layers\n",
    "# # Note: Has to be same length as net_dims. 0 indicates no dropout\n",
    "# drop_prob_list = [0, 0, 0, 0]\n",
    "\n",
    "# # binary list indicating if batchnorm should be implemented for a layer\n",
    "# # The length of the list is equal to the number of layers\n",
    "# # 1 indicates bathnorm and 0 indicates no batchnorm\n",
    "# # If your implementation of batchnorm is incorrect, then set bnorm_list = [0,0,0,0]\n",
    "# bnorm_list = [1,1,1,1]\n",
    "# assert(len(bnorm_list) == len(net_dims)-1)\n",
    "\n",
    "# # list of strings indicating the activation for a layer\n",
    "# # The length of the list is equal to the number of layers\n",
    "# # The last layer is usually a linear before the softmax\n",
    "# act_list = ['relu', 'relu', 'relu', 'linear']\n",
    "# assert(len(act_list) == len(net_dims)-1)\n",
    "    \n",
    "# # initialize learning rate, decay_rate and num_iterations \n",
    "# num_epochs = 3\n",
    "# batch_size = 64\n",
    "# learning_rate = 1e-2\n",
    "# decay_rate = 1\n",
    "# apply_momentum = True\n",
    "\n",
    "# np.random.seed(1)\n",
    "\n",
    "# print(\"Network dimensions are:\" + str(net_dims))\n",
    "# print('Dropout= [{}], Batch Size = {}, lr = {}, decay rate = {}'\\\n",
    "#       .format(drop_prob_list,batch_size,learning_rate,decay_rate)) \n",
    "\n",
    "# # getting the subset dataset from MNIST\n",
    "# trX,tsX, trY,  tsY = train_test_split(\n",
    "#     data, digits.target, test_size=0.5, shuffle=False)# We need to reshape the data everytime to match the format (d,m), where d is dimensions (784) and m is number of samples\n",
    "# trX = trX.reshape(-1, 8*8).T\n",
    "# trY = trY.reshape(1, -1)\n",
    "# tsX = tsX.reshape(-1, 8*8).T\n",
    "# tsY = tsY.reshape(1, -1)\n",
    "# print(trY.shape)\n",
    "\n",
    "# costs, parameters = multi_layer_network(trX, trY, net_dims, act_list, drop_prob_list, bnorm_list,num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate,decay_rate=decay_rate, apply_momentum=apply_momentum, log=True)\n",
    "\n",
    "# # compute the accuracy for training set and testing set\n",
    "# train_Pred = classify(trX, parameters)\n",
    "\n",
    "# test_Pred = classify(tsX, parameters)\n",
    "\n",
    "# # Estimate the training accuracy 'trAcc' comparing train_Pred and trY \n",
    "# # Estimate the testing accuracy 'teAcc' comparing test_Pred and tsY\n",
    "# # your code here\n",
    "# trAcc = np.mean(np.double(train_Pred == trY)) * 100\n",
    "# teAcc = np.mean(np.double(test_Pred == tsY)) * 100\n",
    "\n",
    "# print(\"Accuracy for training set is {0:0.3f} %\".format(trAcc))\n",
    "# print(\"Accuracy for testing set is {0:0.3f} %\".format(teAcc))\n",
    "\n",
    "# plt.plot(range(len(costs)),costs)\n",
    "# plt.xlabel(\"Iterations\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stablepy",
   "language": "python",
   "name": "stablepy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}